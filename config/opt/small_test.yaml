training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  num_train_epochs: 100
  dataloader_num_workers: 16
  fp16: true
  optim: "adamw_torch"
  learning_rate: 0.001
  logging_steps: 100
  save_strategy: "steps"
  save_steps: 4000
  save_total_limit: 1
  deepspeed: config/deepspeed/ds_config_zero1.json
  output_dir: ./output/
  report_to: "wandb"

# model
pretrain_name: facebook/opt-125m
model:
  binary_action_dims:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
  analogue_action_dims: [20, 21]  # camera_0, camera_1

# vision
vision_pretrain_name: openai/clip-vit-base-patch32
vision:
  use_last_projection: True

# vision projection
vision_projection:
  projection_type: 'linear'

  # match them to the hidden dimension of the vision model and the language model
  input_dim: 768
  emb_dim: 768

# processor
preprocessor:
  frame_emb_len: 1

dataset:
  dataset_index_path: data/dataset/tutorial_filtered_xs_v0/dataset.json
  action_dir: data/dataset/tutorial_filtered_xs_v0/idm_actions
  video_dir: data/youtube_videos/tutorial_filtered_xs_v0
  transcripts_dir: data/youtube_transcrips/tutorial_filtered_s_v0

max_length: 200  # TODO: Combine this with dataset field

wandb:
  project: "AlexOPT"
  name: "small_test"

freeze_vision: False