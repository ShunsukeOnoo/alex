# README: Scripts for Data Processing

## classify_frames_local.py
Classify video fraems for filtering out non-play video frames. This script uses the model trained on distillation of OpenAI GPT models. From videos, this script samples frames at a given rate, classify the frames, and stores the results.


Format of the resulting .json files:
```
{
    'video_id': <video_id>,
    'frame_indices': [...]  # list of evaluated frame indices
    'results': [...]        # list of frame classification
    'fps': <fps>            # fps of the video
    'sampling_fps': <sampling_fps>
}
```

Args:
- index_path: path to the index file that stores the list of videos
- video_dir: path to the directory that contains videos
- clip_model_name: the name of the CLIP model used in the distillation model
- clf_model_path: path to the classifier model (saved by `joblib`)
- output_dir: path to the directory to save results
- sampling_fps (int, default 1): the frequency to classify frames
- batch_size (int, default 64): batch size of the classification
- clobber (store_true): whether or not to overwrite results. If not specified, skip videos with existing results.

Note
- It sometimes raises an error `mmco: unref short failure`. But it seems okay for our processing.

To-do
- index fileは使用しなくても問題ないので、変数からはずす
- 出力結果はframe_indicesだけでなく、timestampにしたほうがいい?
- frame_indicesのdtypeをintにする

## make_dataset.py
Based on the filtering results generated by `classify_frames_local.py`, make a dataset. 

The resulting dataset is a .json file, which describes the part of vidoes to be used in the trianing. It has the following format:
```
[
    {
        "video_id": (str) id of the video
        "start_idx": (int) the start index of the clip
        "clip_count": The index of the clip from the same video
        "end_idx": (int) the end index of the clip (this clip is usable)
        "start_timestamp": (float) the start timestamp of the clip
        "end_timestamp": (float) the end timestamp of the clip
        "fps": (int) fps of the video
    },

    # sample
    {
        "video_id": "nqLesE1RiUI",
        "clip_id": 0,
        "start_idx": 480.0,
        "end_idx": 1440.0,
        "start_timestamp": 16.0,
        "end_timestamp": 48.0,
        "fps": 30.0
    },
]
```
Note that a single video may contain multiple clips to use.

Args:
- `filter_result_dir` (str): The directory which contains the results of `classify_frames_local.py`
- `output_dir` (str): The directory to save the results.
- `min_duration` (int, default 4): minimal duration of video clips (in second) to be used in the training.

## Dataset
The dataset `YouTubeDataset` generates `dict` samples with the following keys and values.

- `video_frames` (torch.Tensor): RGB video frames in the clip.
    Shape (n_frames, 3, h, w), Dtype float32, Range [0, 1]
- `frame_timestamps` (torch.Tensor): timestamps for video frames in seconds.
    Shape (n_frames,)
- `actions` (List[dict]): a list of dictionaries containing the actions
- `text` (List[str]): a list of transcript strings
- `text_timestamps` (torch.Tensor): the start time and end time of each transcript.


Note that there are multiple text `str` for one sample, each represents a transcription chunk.
